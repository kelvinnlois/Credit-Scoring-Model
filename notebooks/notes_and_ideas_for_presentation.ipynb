{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e5870a-21c9-4297-b2ab-06cec5f854e0",
   "metadata": {},
   "source": [
    "### Framework for the Slide\n",
    "1. Addressing the coding assignment first\n",
    "    - Explain the data (EDA etc)\n",
    "        - General description and distribution of the data \n",
    "        - outcomes of interest, also called dependent variables, For the PD model, we'll need an indicator or a flag, whether the borrower defaulted or not. Explain the changes of value in target column.\n",
    "        - the rest of our variables are the ones containing the information we will use to predict the dependent variables. These are called independent variables, predictors or features.\n",
    "    - Explain the variables why use Logistic Regression as PD model\n",
    "        - Logistic regression\n",
    "        $$ \\ln(\\text{odds}) = \\ln \\Big( \\frac{\\text{prob_non-default}}{\\text{prob_default}} \\Big) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_m X_m = \\sum_i \\beta_i X_i$$\n",
    "        - Logistic regression is interpretable and easy to undertand and apply.\n",
    "    - Explain PD model\n",
    "        - All variables are categorical variables since we want to create a scorecard. So we need to transform the continuous variables into categorical variables and possibly recategorize some of the discrete variables (why.. ? too many categories or the information they hold about the differences between defaulted and non defaulted loans across some of the categories may be negligible). For continuous, we will start by turning each of them into many categories of equally sized intervals. Once we have created these intervals or classes, we will explore how well each of them discriminates between defaulted and non defaulted loans similar to our discrete variable preprocessing. If two adjacent categories discriminate equally well, we are going to merge them. While if the next category discriminates a lot better or a lot worse than the previous one, it is going to be part of a new variable. There are very rigid rules to these operations and we will get very good at performing them.\n",
    "    - Explain transition PD model $\\to$ Scorecard (simplified model)\n",
    "        - Scorecard much more interpretable and easy to use.\n",
    "        - PD Model should be easy to understand and interpretable\n",
    "        - We can have one-to-one correspondence between probability of default $\\leftrightarrow$ credit scores (credit worthiness fromm scorecard) \n",
    "    - Explain how to calculate credit scores\n",
    "    - Setting Cutoff\n",
    "        - We can also use credit scores to obtain a probality of default of each users.\n",
    "        - PD Model Coeff and Credit Scores both serve the exact same purpose. The only difference is the scale\n",
    "        - We can obtain sum of coeff from credit score for each user/observation\n",
    "        - Cutoff is determined to decide whos is granted a credit and who isn't.\n",
    "        - Cutoff is used for taking decision whether to approve a credit or not\n",
    "        - Once cutoff is chosen, it is predetermines the total number of borrowers that will be approved or rejected. And hence impact the quality of loans.\n",
    "        - __We want the overall default rate of the portfolio to be below 2.5%__. Now from our model, we can obtain the default rate for each _probability threshold (cutoff)_ between 0 and 1. But since we know that there is one-to-one correspondence between probability of default with credit score, then for each default rates thet is generated by the model, have its corresponding credit score cutoff. Hence by looking at all possible cutoff that generate default rate less that 2.5%, we can choose the appropriate cutoff, for example 592.\n",
    "2. Addressing the short-answer assignment\n",
    "3. Next Step :  Do some of them before presentation\n",
    "4. Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020c0cb-4990-481c-85a3-9c361eda8922",
   "metadata": {},
   "source": [
    "### Next Step : What's wrong with my model? How I can improve the model\n",
    "1. Addressing the suitable metric\n",
    "    - Use Precision-Recall curve\n",
    "    - Use Cost-Based Classification\n",
    "    - Training data via undersampling/oversampling\n",
    "    - Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d1545b-fece-4bdb-8385-d2439a8a183b",
   "metadata": {},
   "source": [
    "### Create a presentation script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e2029-5216-4635-92e9-eac23eb20e6a",
   "metadata": {},
   "source": [
    "### Subtleties\n",
    "1. What is reference categories?\n",
    "2. p-value\n",
    "3. Weight of Evidence, Information value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab950ef-cd81-4b11-a945-231646ec2a4c",
   "metadata": {},
   "source": [
    "$$\n",
    "h(x) = \\frac{1}{1+ \\exp^{-\\sum_i \\beta_i X_i}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "J(\\beta) = -y \\, \\ln(h(x)) - (1-y) \\ln(1-h(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{\\text{new}}(\\beta) = - \\color{blue}{w_1}\\, y \\, \\ln(h(x)) - \\color{red}{w_0} \\, (1-y) \\ln(1-h(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffef355-2d31-4f31-8b78-3600804a234a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
